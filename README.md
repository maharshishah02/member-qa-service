# Member QA Service

A lightweight FastAPI application that answers natural-language questions using member messages retrieved from the provided public API.

## Live Deployment:
```
https://maharshi02-member-qa-service.hf.space/chat/
```
## Goal

The service answers natural language questions such as:

â€œWhen is Layla planning her trip to London?â€

â€œHow many cars does Vikram Desai have?â€

â€œWhat are Amiraâ€™s favorite restaurants?â€

Given a question, the API returns:
```
{ "answer": "..." }
```
## Features

âœ… FastAPI endpoint /ask

Accepts a question via query parameter and returns the inferred answer.

âœ… Gradio UI (/chat)

Interactive chat interface for natural-language Q&A.

âœ… Data Source Integration

Reads all member messages from:

```GET https://november7-730026606190.europe-west1.run.app/messages```

âœ… Public Deployment

Docker-based FastAPI Space deployed on Hugging Face.

## API Endpoints

1ï¸âƒ£ Ask a Question

```GET /ask?query=hello```

Example Response:
```
{
  "answer": "Laylaâ€™s trip to London is planned for June 2024."
}
```

2ï¸âƒ£ API Docs (Swagger UI)

```/docs```

3ï¸âƒ£ Gradio Chat UI

```/chat/```

## System Architecture

               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚   /messages API     â”‚
               â”‚  External Data Src  â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                Fetch & Normalize
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚       Indexer / Retriever       â”‚
        â”‚  (semantic search over messages)â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
               LLM Reasoning Layer
                         â”‚
               FastAPI `/ask` Endpoint
                         â”‚
                  JSON Answer Output

## Tech Stack

1. FastAPI â€“ backend API

2. Gradio â€“ chat interface

3. OpenAI model â€“ reasoning engine

4. httpx / requests â€“ data fetching

5. Docker â€“ deployment on Hugging Face

## How It Works

1) Downloads all messages from the /messages API

2) Normalizes & indexes the messages

3) Performs semantic retrieval to find relevant messages

4) Sends question + retrieved context to an LLM

Returns:

```{ "answer": "..." }```

## Bonus 1: Design Notes (Alternative Approaches)

1) Rule-Based Parsing

- Keyword mapping to message fields.

- âŒ Too brittle for free-form natural language.

2) Embedding-Based Semantic Retrieval (Chosen Approach)

- Retrieve relevant messages using vector search.

 âœ”ï¸ Generalizable

 âœ”ï¸ Simple

 âœ”ï¸ Works with varied phrasing

3) Fine-Tuned QA Model

- omain-specific training.

- âŒ Too heavy for assignment scope.

## Bonus 2: Data Insights

From analyzing the member message dataset:

1) Inconsistent date formats

 â€œnext Juneâ€, â€œ6/10/2024â€, â€œJune 2024â€
 â†’ Requires LLM interpretation.

2) Ambiguous references

 â€œher tripâ€, â€œmy carâ€
 â†’ Needs context-based disambiguation.

3) Missing information

 Some messages imply details but never state them explicitly.

4) Name variations

 â€œVikramâ€, â€œVikâ€
 â†’ Must reference same member.

These inconsistencies justify using semantic retrieval + LLM reasoning instead of rule-based parsing.

## Running Locally

Install dependencies:

```pip install -r requirements.txt```


Run development server:

```uvicorn app.main:app --reload```


Open in browser:

API Docs â†’ http://localhost:8000/docs

Chat UI â†’ http://localhost:8000/chat/

## Deployment

This application is deployed using a Dockerized FastAPI Space on Hugging Face.

It exposes FastAPI on port 7860 and automatically loads via:

```uvicorn server:app --host 0.0.0.0 --port 7860```

ğŸ‘¤ Author

Maharshi Shah
