#ğŸ“˜ Member QA Service

A lightweight FastAPI application that answers natural-language questions using member messages retrieved from the provided public API.

ğŸ‘‰ Live Deployment:
```
https://maharshi02-member-qa-service.hf.space/chat/
```
âœ¨ Goal

The service answers natural language questions such as:

â€œWhen is Layla planning her trip to London?â€

â€œHow many cars does Vikram Desai have?â€

â€œWhat are Amiraâ€™s favorite restaurants?â€

Given a question, the API returns:
```
{ "answer": "..." }
```
ğŸš€ Features

âœ… FastAPI endpoint /ask

Accepts a question via query parameter and returns the inferred answer.

âœ… Gradio UI (/chat)

Interactive chat interface for natural-language Q&A.

âœ… Data Source Integration

Reads all member messages from:

```GET https://november7-730026606190.europe-west1.run.app/messages```

âœ… Public Deployment

Docker-based FastAPI Space deployed on Hugging Face.

ğŸ“¡ API Endpoints

1ï¸âƒ£ Ask a Question

```GET /ask?query=hello```

Example Response:
```
{
  "answer": "Laylaâ€™s trip to London is planned for June 2024."
}
```

2ï¸âƒ£ API Docs (Swagger UI)

```/docs```

3ï¸âƒ£ Gradio Chat UI

```/chat/```

ğŸ§  System Architecture

               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚   /messages API     â”‚
               â”‚  External Data Src  â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                Fetch & Normalize
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚       Indexer / Retriever       â”‚
        â”‚  (semantic search over messages)â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
               LLM Reasoning Layer
                         â”‚
               FastAPI `/ask` Endpoint
                         â”‚
                  JSON Answer Output

ğŸ”§ Tech Stack

FastAPI â€“ backend API

Gradio â€“ chat interface

OpenAI model â€“ reasoning engine

httpx / requests â€“ data fetching

Docker â€“ deployment on Hugging Face

ğŸ§ª How It Works

Downloads all messages from the /messages API

Normalizes & indexes the messages

Performs semantic retrieval to find relevant messages

Sends question + retrieved context to an LLM

Returns:

```{ "answer": "..." }```

â­ Bonus 1: Design Notes (Alternative Approaches)

1) Rule-Based Parsing

Keyword mapping to message fields.

âŒ Too brittle for free-form natural language.

2) Embedding-Based Semantic Retrieval (Chosen Approach)

Retrieve relevant messages using vector search.

âœ”ï¸ Generalizable

âœ”ï¸ Simple

âœ”ï¸ Works with varied phrasing

3) Fine-Tuned QA Model

Domain-specific training.

âŒ Too heavy for assignment scope.

â­ Bonus 2: Data Insights

From analyzing the member message dataset:

1) Inconsistent date formats

â€œnext Juneâ€, â€œ6/10/2024â€, â€œJune 2024â€
â†’ Requires LLM interpretation.

2) Ambiguous references

â€œher tripâ€, â€œmy carâ€
â†’ Needs context-based disambiguation.

3) Missing information

Some messages imply details but never state them explicitly.

4) Name variations

â€œVikramâ€, â€œVikâ€
â†’ Must reference same member.

These inconsistencies justify using semantic retrieval + LLM reasoning instead of rule-based parsing.

ğŸ—ï¸ Running Locally

Install dependencies:

```pip install -r requirements.txt```


Run development server:

```uvicorn app.main:app --reload```


Open in browser:

API Docs â†’ http://localhost:8000/docs

Chat UI â†’ http://localhost:8000/chat/

ğŸ“¦ Deployment

This application is deployed using a Dockerized FastAPI Space on Hugging Face.

It exposes FastAPI on port 7860 and automatically loads via:

```uvicorn server:app --host 0.0.0.0 --port 7860```

ğŸ‘¤ Author

Maharshi Shah
